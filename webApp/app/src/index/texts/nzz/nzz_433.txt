Ingenieure wissen es längst: Der Computer ist die Lösung. Was uns jetzt noch fehlt  ist das Problem.*1983 gründeten der MIT-Absolvent und Computerwissenschaftler Danny Hillis und die Architektin und Mitgründerin einer Gentechnik-Firma Sheryl Handler die Firma „Thinking Machine“. Hillis träumte davon  eine Maschine zu bauen  die denken kann und stolz auf ihre Erbauer ist.Seine Doktorarbeit am MIT befasste sich mit Parallelverarbeitung. Computer arbeiteten zu der Zeit seriell  einen Schritt nach dem anderen. In der „Connection Machine“ (CM)  die Thinking Machine baute  waren bis zu 65.536 Prozessoren gleichzeitig am Werk. Und: Endlich sahen Computer auch in der Wirklichkeit so aus  wie sie im Kino schon lange aussahen. Die CM bestand aus düster-eleganten Kuben in einer Farbe  die firmenintern „Darth-Vader-black“ genannt wurde  übersät mit roten Leuchtdioden  die hypnotische Lichtmuster erzeugten; jede Diode zeigte die Aktivität eines der Prozessoren in der Maschine an.Steven Spielberg war so eingenommen von der Technologie  dass er für die Rolle des Supercomputers in seinem Film Jurassic Park eine Connection Machine vorsah  obwohl in dem zugrundeliegenden Roman von Michael Crichton ein Cray-Supercomputer benannt ist. Aber als mit dem Ende des Kalten Kriegs Anfang der Neunzigerjahre die Subventionen der DARPA  der Entwicklungsabteilung des US-Verteidigungsministeriums  ausblieben und Firmen wie IBM und Intel in den Markt eintraten  wurde es eng. 1994 – in dem Jahr  in dem das Internet an Fahrt aufnahm – musste Thinking Machines Konkurs anmelden. In den 11 Jahren ihres Bestehens hatte sie 112 ihrer Supercomputer verkauft  zwei beispielsweise an American Express  um Kreditkartendaten in zuvor nicht gekannter Geschwindigkeit nach neuen Informationen über ihre Kunden durchzugraben.Und Hillis hatte sich  zwar nicht wirtschaftlich  aber konzeptuell weitsichtig zur Zukunft des Computerns geäussert. So stellte er sich eine künftige  leistungsfähigere Version seiner Maschine als eine öffentliche Intelligenz-Ressource vor  die der Welt den Zugang zu künstlicher Intelligenz ermöglichen sollte  so wie man Wasser und Strom bezieht (und der das  was wir heute Cloud Computing nennen  schon recht ähnlich sieht).Hillis wies darauf hin  dass es technisch machbar sei  eine tausendmal grössere Maschine zu bauen als die Connection Machine (oder wie der Computerkonstrukteur Burton J. Smith selbstironisch auf dem Branchentreffen Supercomputing ’91 anmerkte: „Wir können heute Rechner bauen  die niemand bezahlen kann.“) Eine solche Maschine wäre gross wie ein Haus geworden  aber die ersten Computer in den fünfziger Jahren hatten auch ganze Etagen eingenommen.Das eigentliche Problem aber waren – und sind – die Probleme. Die meisten computergerecht formulierbaren Probleme seien angesichts der Leistungsfähigkeit einer solchen Maschine trivial.Es muss also ein neues Berufsbild her: der Problemdesigner. Der Experte  der computergerechte Probleme massschneidern kann. Der die algorithmisierbaren Teile der Realität erkennt und in Programme umsetzt und der das Nichtalgorithmisierbare  das Wilde an der Welt  minimiert  interpoliert und ungefährlich macht.*Der Begriff lässt kaum Raum für Zweifel: Supercomputer machen grosse Dinge.Im März 2002 etwa ging in Yokohama der Earth Simulator in Betrieb. Für den damals schnellsten Supercomputer der Welt wurde ein 3.250 Quadratmeter grosses Gebäude errichtet  die halbe Fläche eines Fussballfelds. Die Maschine schaffte bis zu 40 Billionen Rechenschritte in der Sekunde  technically spoken 40 Teraflops (FLOPS steht für „floating point operations per second” – Rechenschritte mit Gleitkommazahlen).Bis zum Juni 2004 hielt das Grossgerät den Spitzenplatz als schnellster Rechner der Welt. In der Zeit diente es der Erforschung des Klimawandels und möglicher Lösungen für globale Umweltprobleme. Anhand eines „virtuellen Planets Erde” wurden langfristige geophysikalische  klimatische und wetterbedingte Phänomene nachgebildet. Grosse Sache.2008 war die Maschinenhalle auf Platz 49 der jährlich aktualisierten Liste der TOP 500 der weltweit schnellsten Supercomputer abgerutscht und wurde einer Auffrischung unterzogen. Platz 1 auf der Liste nimmt derzeit der aus 3 12 Millionen Prozessorkernen bestehende Computercluster Tianhe-2 („Milchstrasse-2“) des National Super Computer Center im chinesischen Guangzhou ein.*Anfang der Neunzigerjahre hatte Allan Bromley  der Wissenschaftsberater des damaligen US-Präsidenten Bush  unter dem Titel „Grand Challenges“ eine Liste mit Problemen vorgestellt  zu deren Lösung ein Vielfaches der zu der Zeit bereitstehenden Computerleistung benötigt würde  und einen Etat in Höhe von drei Milliarden Dollar beantragt und genehmigt bekommen. Auf der Liste standen Fragen wie die nach den Formprinzipien von Galaxien oder weshalb das Universum so aussieht  wie es aussieht. Grosse Sache.Um die Jahrtausendwende gab es eine erste Annäherung an eine Antwort auf die Frage nach dem Universum: die Millennium-Simulation  gemeinsam durchgeführt von Kosmologen aus Deutschland  Grossbritannien  Kanada  Japan und den USA  zeigte erstmals  wie aus kleinen „Störungen“ nach dem Urknall grosse Unregelmässigkeiten entstehen  wie wir sie aus dem heutigen Universum kennen. 2010 – weitere zehn Jahre später – zeigt die auf dem Plejades-Supercomputer des Ames Research Center der NASA durchgeführte Bolshoi-Simulation die bislang akkuratesten Ergebnisse dieser Computernachbildung der Entstehung von überhaupt allem. (Die isländische Sängerin Björk verwendete Bilder daraus bei der Aufführung ihres Songs „Dark Matter“.)Das eigentlich Erstaunliche ist: Man staunt nicht. Naja  eine Urknallsimulation. Es ist die Mischung aus Dummheit und Geschwindigkeit  die uns an Computern beeindruckt. Ein Supercomputer kann ja nicht einmal Kaffee holen. Im Mai 1997 wurde Garri Kasparov von dem Supercomputer „Deep Blue“ besiegt. Zum ersten Mal unterlag ein Schachweltmeister einer Maschine. Und? Nichts. Danach stand Deep Blue in einer Lagerhalle in New Jersey und machte Data Mining  grub grosse Datenbeständen nach unentdeckten Mustern um  mit denen sich wirtschaftliche Abläufe einträglicher gestalten lassen. Ein bisschen  als würde ein vormals grosser Mann sich auf der Strasse nach Kleingeld bücken müssen.Der Plejades-Supercomputer der NASA  auf dem die Bolshoi-Simulation gerechnet wurde  gehört zur Altix-Familie von Hochleistungsrechnern der Firma Silicon Graphics. Werden Systeme der Altix-Familie mit einem Grafikmodul ausgestattet  heissen sie Prism.*Das zufällig namensgleiche NSA-Überwachungsprogramm PRISM hat seinen Anfang im Jahr 2005 genommen  also genau zwischen den beiden Supercomputer-Simulationen des Universums. Mit PRISM wird die digitale Kommunikation von Menschen innerhalb und ausserhalb der USA umfassend überwacht. Enthüllt wurde dieses Programm  das zum Synonym geheimdienstlicher Methoden im Internet-Zeitalter wurde  von Edward Snowden.Hier nun sind die Problemdesigner an der Arbeit  im Utah Data Center beispielsweise  das die NSA seit 2013 nahe einer Stadt mit dem schönen Namen Bluffdale betreibt. Das Gelände mit dem wurstförmigen Umriss  auf dem der womöglich grösste und leistungsfähigste Rechnerpark der Welt läuft  umfasst 93.000 Quadratmeter  es hätten also 28 Earth Simulatoren darauf Platz. Die Leistungsfähigkeit der Anlage lässt sich nur schätzen. Umgerechnet auf die Weltbevölkerung soll ihre Speicherkapazität einem Datenvolumen von zwischen 1 4 Megabyte und 140 Terabyte pro Person entsprechen. „Damit“  merkt die deutschsprachige Wikipedia lapidar an  „wird der Schritt in die komplette Überwachung und Speicherung der weltweiten Kommunikation möglich.“„Kultur ist Reichtum an Problemen“  schrieb Egon Friedell schon vor 100 Jahren  „und wir finden ein Zeitalter um so aufgeklärter  je mehr Rätsel es entdeckt hat.“ Womit er  von Hand berechnet und weitreichend  wie wirkliche Poesie nun einmal ist  bereits die Arbeitsgrundlage der Code- und Verfassungsknacker des 21. Jahrhunderts erfasst hat.*
 
